---
description: Core Engineering Principles
alwaysApply: true
---

# Core Engineering Principles

These fundamental principles guide all development work and must be considered in every decision.

## Product & User Principles

- **Relentlessly User-Centric**
  Prioritize understanding and addressing user needs. Product quality, developer productivity, and job satisfaction are all downstream effects of this focus. Success is determined by the value delivered to users.
- **Stable Priorities**
  Maintain stable, clear organizational priorities. Constant churn is a primary cause of burnout and must be actively managed and mitigated by leadership.

## Delivery Principles

- **Maintain Small Batch Sizes**
  This is the most critical delivery principle. Large changes are the primary cause of reduced delivery stability and throughput. All work is broken down into the smallest possible units that can be independently deployed and deliver value.
- **Trunk-Based Development**
  All work integrates into the `main` branch continuously. Avoid long-lived feature branches to prevent integration complexity and support small batch sizes.
- **Comprehensive Automation**
  The path to production must be fully automated. This includes all testing, quality gates, and infrastructure provisioning. Manual steps are an anti-pattern.
- **Promote Decoupled Deployment & Release**
  Champion the use of feature flags to separate the act of deploying code to production from the act of releasing features to users. This reduces risk and enables progressive rollouts.

## Architectural Principles

- **Stateless Services**
  Services must not maintain internal state. State externalizes to dedicated persistence layers.
- **Idempotent Operations**
  Design all API endpoints and event handlers to be retried without unintended side effects.
- **Design for Failure**
  Assume every component and dependency will fail. Implement mechanisms for graceful degradation, retries (with backoff), and circuit breakers.
- **Embedded Observability**
  Systems design for observability from the start, including structured logging, key metrics, and distributed tracing.
- **Adhere to DRY (Knowledge, Not Code)**
  Every piece of knowledge must have a single, unambiguous representation.
- **Employ Asynchronous Communication**
  Use message queues or event streams for non-blocking operations to enhance scalability and resilience.
- **Demand Loose Coupling**
  Components must interact through stable, well-defined interfaces (APIs, events). This allows components to develop, deploy, and scale independently.

## Implementation Principles

- **Embrace Functional & Immutable Patterns**
  Utilize immutable data structures and functional operators (for example, map, filter, reduce) to promote immutability, reduce side effects, and improve code readability.
- **Prioritize Type Safety**
  Avoid language features that disable type safety (for example, `any` in TypeScript). For values of an unknown type, use explicit type-narrowing checks.
- **Favor Interoperable Language Features**
  Choose language features and patterns that are interoperable and stable across various versions of a language or different execution environments, promoting long-term maintainability and reducing compatibility issues.
- **Clarity Over Cleverness**
  Prioritize clarity, commonality, and composability over conciseness or single-line power. Build rich behavior by composing simpler, more widely understood constructs.

## Evidence-Based Engineering

All engineering work must be grounded in evidence, experimentation, and verifiable claims. This applies to architecture decisions, implementation choices, performance assertions, and documentation.

- **No Unqualified Assumptions**
  Never make assertions about performance, behavior, scalability, or capabilities without:
  - Measured data from experiments or prototypes
  - Citations to authoritative sources (documentation, research papers, benchmarks)
  - Explicit labeling as targets, estimates, or hypotheses to be validated
- **Distinguish Facts from Targets**
  Clearly differentiate between:
  - **Verified facts**: Measured results from implementation or cited from authoritative sources
  - **Targets**: Performance goals or requirements to be validated through implementation
  - **Hypotheses**: Educated assumptions that require experimental validation before being treated as fact
  - **Constraints**: Known limitations from platform documentation or requirements
- **Experiment Before Committing**
  For critical architectural decisions or performance-sensitive implementations:
  - Build prototypes or spikes to validate assumptions
  - Measure actual performance on target hardware
  - Document experimental results that informed decisions
  - Use profiling tools and benchmarks rather than intuition
- **Cite Sources**
  When referencing framework capabilities, algorithms, standards, or best practices, include links to:
  - Official documentation
  - Research papers or technical articles
  - Benchmark results or case studies
  - Community consensus (with appropriate caveats)
- **Avoid Precision Without Proof**
  Do not use specific numbers (for example, "95% accuracy", "<5 seconds", "~50 MB", "10x faster") unless they are:
  - Measured from actual implementation or prototype
  - Cited from authoritative sources with context
  - Explicitly labeled as targets or estimates to be validated
- **Qualify Performance Claims**
  When discussing performance:
  - Specify test conditions (hardware, data size, environmental factors)
  - Use qualitative language ("responsive", "efficient", "scalable") when specific metrics are unavailable
  - Frame unvalidated claims as targets: "target 60 FPS" not "achieves 60 FPS"
  - Document performance degradation conditions
- **Challenge Assumptions**
  Regularly question and validate assumptions through:
  - Code reviews that ask "how do we know this?"
  - Performance testing on minimum supported hardware
  - User testing to validate usability assumptions
  - Load testing to validate scalability claims
